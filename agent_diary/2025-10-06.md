# 2025‑10‑06 — Daily Log

## Focus

Stand up customizable inference workflows that respect live portfolios, add valuation reporting, and refresh evaluation artifacts.

## Context

* Yesterday I clarified training timestep accounting and stabilized evaluation traces with explicit trading dates.
* Regeneration of evaluation outputs was pending because the RL environment lacked key dependencies.
* Today I will implement inference overrides, plotting, and regressions, then reprovision the tooling to regenerate artifacts.

---

## Design Notes

* Extend `MultiStockTradingEnv.reset` to accept **portfolio‑state overrides** (`cash`, `holdings`, allocation weights, optional start index) **without clobbering history buffers**.
* Provide translation helpers that **map allocation weights to integer share positions** given current prices and leverage constraints.
* Build an **inference pipeline** that loads checkpoints, applies the overrides, and emits updated state snapshots alongside run metrics.
* Hook **valuation curve rendering** onto the evaluation/inference path, annotating peak NAV and storing plots next to CSV traces.
* **Recreate** the RL environment with required dependencies and **rerun evaluation scripts** to refresh artifacts, then codify **monotonic‑date** expectations through regression tests.

---

## Decision Table (Final Choices)

| Item                                   | Default (Recommended)                                                                                 | Rationale (Core Argument)                                                                                            | Alternative & When                                                                                                                      |
| -------------------------------------- | ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Cost timing (buy/sell)**             | **Charge immediately (one‑way)**: debit buy cost on buys, sell cost on sells                          | Clear credit assignment (cost applied right after the action), discourages over‑trading, minimizes reward delay/bias | *Entry reservation (half round‑trip)*: add `entry_extra = 0.5*c*buy_notional` when you want a higher barrier to entry (default **OFF**) |
| **Cost basis `V_basis`**               | **post‑return**: `V_basis = V_gross` (value after applying returns)                                   | Closer to available capital at fill time; naturally accounts for rebalancing notches                                 | *pre‑return*: `V_prev` lowers variance / increases stability. Use temporarily if early training is unstable                             |
| **Cost intensity**                     | **`c = 0.005` (0.5%) per side**                                                                       | Problem assumption: strong‑cost environment. Round‑trip ≈ 1.0%                                                       | For cost ablations, ramp `c: 0 → 0.005` (curriculum)                                                                                    |
| **Action space**                       | **Softmax weights (N stocks + cash)** + **transition limit τ**                                        | Continuous space plays well with PPO; avoids combinatorial explosion                                                 | Log‑ratio / clamped transforms offer little observed gain—keep baseline                                                                 |
| **Transition limit τ**                 | **τ: 0.10 → 0.15 (final)**                                                                            | With 0.5% costs, over‑trading control matters. Too high ⇒ turnover ↑                                                 | More conservative: keep 0.10. More aggressive: 0.20 (if costs ↓)                                                                        |
| **Extra trading penalty `λ_turnover`** | **`λ = 0.02`** (light shaping)                                                                        | Cost hits wealth, but this early nudge suppresses “action spam” during exploration                                   | Remove (0.0) when cost alone suffices; avoid ≥0.05 (over‑penalization)                                                                  |
| **Reward**                             | **`log(V_t/V_{t-1}) - λ*turnover`**                                                                   | Sums to geometric return, scale‑stable, good credit assignment                                                       | “Sharpe‑like” standardized reward is better handled via `VecNormalize`                                                                  |
| **Episode length**                     | **126d (train)** / **252–504d (eval)**                                                                | Enough signal + path dependence; short episodes bias to turnover                                                     | Warm‑up 60–90d for curriculum only                                                                                                      |
| **Normalization**                      | **`VecNormalize(obs/reward)=ON`**, clip ±10                                                           | Critical for PPO convergence—stabilizes scale volatility                                                             | OFF prohibited (many failure cases)                                                                                                     |
| **Integerization (train/eval)**        | **Train continuous**, **eval/production: integerization execution layer**                             | Keeps training stable and deployment realistic; next observation uses `w_exec`                                       | During training, optionally inject stochastic rounding with prob `p` for robustness                                                     |
| **Return/cost ordering**               | **(1) Realize returns `(w_prev, r_t)` → (2) Debit costs `c·turnover·V_gross` → (3) State transition** | Avoids information leakage; matches fill ordering                                                                    | Keep as‑is (do not break consistency)                                                                                                   |

---

## Final Spec

### 1) Transaction cost / slippage — **0.5% per side** charged immediately (default)

* `c = 0.005` captures fees + slippage via a **single parameter**.
* **Buy**: subtract `buy_notional * c` immediately.
  **Sell**: subtract `sell_notional * c` immediately.
* When treating rebalancing via **turnover**:

  ```python
  turnover = 0.5 * np.abs(w_t - w_{t-1}).sum()  # include cash
  V_gross  = V_prev * (1 + np.dot(w_prev[:-1], r_t))
  cost     = c * turnover * V_gross             # post-return basis
  V_t      = max(V_gross - cost, 1e-8)
  ```
* **Entry reservation (optional, OFF by default)**: when entering a position, add
  `entry_extra = 0.5 * c * buy_notional` to suppress churn (use only for extreme churn control).

### 2) Action space + transition constraint

```python
a = policy(obs)                   # [N+1]
w = softmax(a)                    # sums to 1, last index is cash
delta = w - w_prev
tau = schedule(epoch)             # 0.10 → 0.15
scale = min(1.0, tau / (0.5*np.abs(delta).sum() + 1e-8))
w_t = w_prev + scale * delta
turnover_t = 0.5 * np.abs(w_t - w_prev).sum()
```

### 3) Reward

```python
reward_t = np.log(V_t / V_prev) - 0.02 * turnover_t  # λ=0.02
# Use VecNormalize for reward normalization (clip ±10)
```

### 4) Observations / normalization

* Observations: per‑symbol **last k=32 log‑returns**, `w_{t-1}` (N+1), cash weight, (optional) previous turnover.
* `VecNormalize(norm_obs=True, norm_reward=True, clip_obs=10, clip_reward=10)` **required**.
* **Freeze** the scaler between training and evaluation for consistency.

### 5) PPO settings

* `n_envs=16`, `n_steps=512` → batch = 8192
* `gamma=0.99`, `gae_lambda=0.95`, `clip_range=0.2`
* `ent_coef=0.01` (start higher → decay), `vf_coef=0.5`, `max_grad_norm=0.5`
* `learning_rate=3e-4` (cosine/linear decay), policy MLP `[256, 256]` (Tanh, orthogonal init)
* `target_kl≈0.02`, `VecNormalize` **ON**

### 6) Curriculum

1. Single stock, `c=0` → verify convergence
2. Multi‑stock, `c=0`
3. **Cost ramp**: `c: 0 → 0.005` over 5–10 epochs
4. **τ ramp**: `0.10 → 0.15`
5. Random start days / sliding‑window sampling

### 7) Execution layer (eval/production) — **integerization**

* **Train continuous**, **eval/live** use an **execution rounding layer** (largest‑remainder + cost awareness). Next observation uses `w_exec` for consistency.

```python
def round_to_lots(w_target, V, C, q_prev, p, c):
    buffer = max(c*0.5, 0.001)
    V_eff  = V * (1 - buffer)
    d_tgt  = w_target[:-1] * V_eff
    h      = d_tgt / p
    q      = np.floor(h).astype(int)

    cash = C + np.sum(np.maximum(0, q_prev - q) * p * (1 - c))
    need = np.sum(np.maximum(0, q - q_prev) * p * (1 + c))
    if need > cash:  # shrink most expensive first
        for i in np.argsort(-(p * (1 + c))):
            while q[i] > q_prev[i] and need > cash:
                q[i] -= 1
                need -= p[i] * (1 + c)

    cash -= need
    frac = h - np.floor(h)
    for i in np.argsort(-frac):     # allocate remainder by largest fraction
        unit = p[i] * (1 + c)
        if cash >= unit:
            q[i] += 1
            cash -= unit

    w_exec = (q * p) / ((q * p).sum() + cash + 1e-8)
    return q, w_exec, cash
```

* For robustness, optionally **inject stochastic rounding** during training with small probability `p`.

### Example config (`configs/ppo.yaml`)

```yaml
algo: PPO
n_envs: 16
n_steps: 512
batch_size: 8192
gamma: 0.99
gae_lambda: 0.95
clip_range: 0.2
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
learning_rate: 3.0e-4
lr_schedule: cosine
target_kl: 0.02
normalize_obs: true
normalize_reward: true
obs_clip: 10.0
rew_clip: 10.0

env:
  window: 32
  tau_schedule: [0.10, 0.15]     # start, final
  lambda_turnover: 0.02
  cost_per_side: 0.005           # fee+slippage (one-way)
  cost_basis: post               # post-return (V_gross)
  entry_reserve: false           # entry reservation OFF
```

### Step order (to avoid information leakage)

1. Observe `obs_t` (information up to time `t`)
2. Policy → `w_t` (apply τ)
3. **Realize returns**: `V_gross = V_prev*(1 + w_prev·r_t)`
4. **Debit costs**: `cost = c*turnover*V_gross`; `V_t = V_gross - cost`
5. Reward: `log(V_t/V_prev) - λ*turnover`
6. Transition state + logging

### Unit test essentials

* **No double‑charging** of costs (don’t combine fill price adjustment with turnover penalty).
* Always enforce `||w_t - w_{t-1}||_1 / 2 ≤ τ`.
* **Scale invariance**: log‑reward unchanged under constant price scaling.
* `c=0` & hold‑only ⇒ reward ≈ 0.
* With `c=0`, PPO should **improve** rewards (baseline sanity).

### Failure diagnostics → remedies

* **Excess cash / timid actions**: temporarily lower `c`, raise `τ` (≤0.20), or increase `ent_coef`.
* **KL ≈ 0**: raise `target_kl` or increase `lr` / `ent_coef`.
* **KL too high**: lower `clip_range`, lower `lr`.
* **Turnover too high**: lower `τ`, raise `λ` (0.03–0.05), add min trade size.
* **High variance**: temporarily switch `cost_basis=pre` to stabilize; revert to `post` later.

---

## Implementation Log (today’s progress and recent changes)

* Integrated **portfolio overrides** into `MultiStockTradingEnv` with queued state application, symbol‑index lookup, leverage guardrails, and baseline normalization tweaks. (`src/env/multi_stock_env.py`)
* Added a **customizable inference runner + CLI** so checkpoints load with user‑specified holdings/allocations via JSON payloads; wired overrides through **deterministic evaluation**. (`src/pipelines/evaluation.py`, `src/pipelines/inference.py`)
* Extended the **evaluation stack** with **valuation plot generation** (peak NAV annotations, per‑episode charts, optional CLI toggle) stored alongside trace artifacts. (`src/pipelines/evaluation.py`, `src/pipelines/inference.py`)
* **Reprovisioned** the RL toolchain on **Python 3.13** using **Torch 2.6.0**, **Stable‑Baselines3 2.7.0**, Gymnasium, Matplotlib, PyYAML, and PyTest (worked around pinned Torch 2.4.1 incompatibility; documented a compatible wheel swap).
* Executed the **new inference CLI** against `trial_v3_01_ppo` with custom allocations JSON; captured refreshed traces and valuation plots under `artifacts/results/inference_trial_v3_custom`. (`src/pipelines/inference.py`)
* Added **regression coverage** to ensure evaluation traces remain **date‑monotonic** using a stubbed policy and a dummy dataset. (`tests/test_evaluation_monotonic.py`)
* **Hardened portfolio overrides** to demand **explicit per‑symbol share counts + cash** (no implicit allocations). Added environment tests. (`src/env/multi_stock_env.py`, `tests/test_env.py`)
* Wired **per‑epoch evaluation** to drop valuation PNGs beside CSV traces and persist `.pt` checkpoints on every cadence. (`src/pipelines/training.py`)
* Ran `debug_two_epoch_v2` training (4,096 steps) to produce two evaluation cycles, checkpoint drops, and profit charts under `validation_traces/`.
* Generated a **full‑coverage holdings JSON** (every symbol enumerated) and executed inference with the best checkpoint; produced NAV outputs under `artifacts/results/inference_debug_two_epoch`.

> **Note on overrides vs. allocations:** The **override API now requires explicit integer shares + cash** for reproducibility. The **translation helper remains available outside the env** (see *Execution layer — integerization*) to convert target weights to integer positions before calling `reset` with overrides.

---

## Checklist

* [x] Implement portfolio‑state overrides in `MultiStockTradingEnv` and wire through the inference pipeline.
* [x] Build the dedicated inference entry point that accepts custom holdings/allocation configs and runs deterministic rollouts from checkpoints.
* [x] Generate and store valuation‑curve plots with peak NAV annotations alongside evaluation artifacts once customized inference is in place.
* [x] Recreate the RL environment with required deps (stable_baselines3, pytest) before executing the new inference/evaluation flows.
* [x] Rerun `scripts.evaluate` (or the new inference CLI) to refresh traces after environment reprovisioning.
* [x] Backfill regression tests enforcing **monotonic evaluation dates** given the updated metadata path.
* [x] Require explicit per‑symbol share counts (including zeros) in portfolio state overrides and validate via automated tests.
* [x] Complete a two‑project‑epoch debug training run with evaluation‑trigger checkpoints and co‑located profit charts for each epoch.
* [x] Run inference on the best debug checkpoint using the full holdings manifest and archive outputs/plots.

---

## Gaps / Outstanding Work

* **Spec integration:** Ensure the **immediate one‑way cost** and **post‑return cost basis** are the **single source of truth** across env, evaluation, and inference (no lingering legacy paths that double‑charge or use pre‑return).
* **τ constraint tests:** Add **property‑based tests** to stress the τ limiter under random `w`/`w_prev` (corner cases near τ).
* **VecNormalize parity:** Guarantee the **frozen scaler** is loaded during inference/evaluation; add a guard that fails fast if mismatched stats are detected.
* **Integerization in eval path:** Confirm the **execution rounding layer** is actually invoked in evaluation/inference when `--integerize` is set; log `q`, `w_exec`, and residual cash.
* **Docs & configs:** Promote `configs/ppo.yaml` to the **canonical** config; document `entry_reserve` toggle and the curriculum ramp for `c` and `τ`.
* **CLI UX:** Add validation for holdings JSON (symbol coverage, non‑negativity, cash consistency) and a helper to **derive a full holdings JSON from target weights** for convenience.

---

## Today’s Plan (Agent, do this)

1. **Prototype & test the `portfolio_state` API** end‑to‑end:

   * Unit tests for queued state application, symbol mapping, and history buffer integrity.
   * Negative tests for partial symbol coverage or missing cash field.
2. **Inference CLI** finalization:

   * Verify arbitrary holdings configurations are respected deterministically.
   * Add `--integerize` toggle and pipe through the rounding layer; emit `q`, `w_exec`, `cash_end`.
3. **Valuation plotting** polish:

   * Peak NAV annotations, per‑episode PNGs next to CSVs, CLI on/off flag.
   * Embed run metadata (commit, seed, `c`, `τ`) into plot footer for traceability.
4. **Spec convergence & regressions**:

   * Enforce **post‑return**, **immediate one‑way cost** ordering everywhere (single helper).
   * Add τ‑constraint property tests; expand monotonic‑date tests with boundary dates and DST edges.
5. **Tooling reprovision & artifact refresh**:

   * Lock dependency versions (Py3.13, Torch 2.6.0, SB3 2.7.0) in a reproducible manifest.
   * Rerun evaluations with the best checkpoint and integerization ON; archive traces + plots.

### Acceptance Criteria for today

* ✅ Inference run with a custom holdings JSON **reproduces** deterministic NAV and **saves** `{csv, png, q.json}` per episode.
* ✅ All tests pass, including **τ limiter** and **date‑monotonic** checks.
* ✅ `VecNormalize` stats are **consistent** across train/eval; mismatch triggers an explicit error.
* ✅ A single **cost accounting** path is used (post‑return, immediate one‑way), verified by unit tests guarding against double‑charging.

---

## Summary

* Locked in a **cost‑aware, τ‑constrained softmax** policy with **log‑wealth** rewards and light turnover shaping, **VecNormalize ON**, and **integerized execution** for evaluation.
* The environment now supports **explicit portfolio overrides**, and the pipeline emits **valuation plots** next to CSV traces.
* Dependencies have been **reprovisioned**; monotonic‑date behavior is covered by tests.
* Today’s work stitches the **final spec** into code paths, **hardens tests**, and ensures **deterministic, integerized** evaluation with clean artifacts.
