# 2025-10-03

## Focus
Establish reliable data/schema foundations, environment scaffolding, and a resumable PPO training pipeline with clear logging.

## Progress
- Captured the processed-table schema (`docs/data_schema.md`) and enforced it in code/tests (`src/data/schema.py`, `tests/test_data_schema.py`), including a validation notebook and `pytest` dependency for quick CSV audits.
- Hardened the dataset loader with schema checks plus per-symbol lag features, trading-gap masks, and per-window normalization (`src/data/loader.py`, `src/data/transforms.py`); documented the strategy and updated `configs/base.yaml` accordingly.
- Stood up the environment stack: friction/reward helpers, sliding-window adapter, and deterministic unit tests covering commission, slippage, and reward shaping (`src/env/*`, `tests/test_env.py`).
- Refreshed builders and config utilities to wire datasets, monitored environments, agent specs, and experiment loggers (`src/utils/builders.py`, `src/utils/config.py`).
- Implemented structured experiment logging with `.log` files, CSV telemetry, and config snapshots for each run (`src/loggers/experiment.py`).
- Delivered a resumable PPO training CLI: windowed data adapter, monitoring callback, torch checkpoints bundling config+state, evaluation hooks, and new tests validating logging and resume flows (`src/pipelines/training.py`, `scripts/train.py`, `tests/test_training_pipeline.py`).
- Updated project checklist, plan milestones, and diary to reflect the end-to-end workflow; all tests now pass (`python -m pytest tests`).

## Findings & Decisions
- Ratio and flow fields remain bounded, so per-window z-score normalization plus lagged log returns capture momentum without cross-stock leakage.
- Structured builders keep config-driven wiring testable, and adding a Monitor wrapper ensures stable episode metrics for SB3 callbacks.
- Checkpointing via `torch.save` with config payloads enables seamless resume and keeps artifacts organized under timestamped run directories.

## Next Steps
1. Build evaluation/reporting scripts that summarise the new logs and checkpoints into quick performance tables.
2. Enrich training metrics (NAV, turnover, drawdown) and surface them in both CSV and `.log` summaries.
3. Expand agent configs (SAC/DDPG) and prepare hyperparameter sweep hooks for upcoming experiments.
