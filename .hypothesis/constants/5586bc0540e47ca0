# file: /mnt/c/Users/rh987/OneDrive/document/GitHub/multistockRL/src/pipelines/training.py
# hypothesis_version: 6.140.3

[0.0, 1e-08, 1.0, 60.0, ', ', '--config', '--resume', '--run-name', '--timesteps', 'Path to config file', 'Run name for logging', 'Training complete', '__main__', '_n_updates', 'algorithm', 'alias', 'best', 'checkpoint_freq', 'checkpoints', 'configs/base.yaml', 'date', 'date_column', 'datetime64[ns]', 'dev', 'drawdown', 'early_stop_epoch', 'early_stop_patience', 'early_stop_reason', 'episode_final', 'episode_steps', 'episodes', 'episodes_completed', 'eval_episodes', 'eval_freq', 'eval_interval_epochs', 'eval_std_reward', 'final_checkpoint', 'final_eval_traces', 'frequency', 'gradient_steps', 'infos', 'last_checkpoint', 'last_eval_epoch', 'last_train_reward', 'last_valid_return', 'last_valid_reward', 'log_interval', 'log_interval_epochs', 'log_interval_steps', 'max_drawdown', 'max_steps', 'n_steps', 'nav', 'no tracked metrics', 'num_timesteps', 'patience', 'planned_epochs', 'planned_timesteps', 'policy', 'portfolio_value', 'ppo_dev', 'project_epoch', 'project_epoch_unit', 'project_epoch_value', 'reward', 'rollback_patience', 'rolled_back_at', 'run_completed', 'size', 'start_timesteps', 'step', 'steps', 'stop_reason', 'total_env_steps', 'total_epochs', 'total_timesteps', 'train', 'train/n_updates', 'train_epochs', 'train_epochs_overrun', 'train_freq', 'train_reward', 'train_timesteps', 'turnover', 'unit', 'updates_applied', 'updates_per_step', 'valid_return', 'valid_reward', 'validation', 'validation_traces', 'vecnormalize', 'vecnormalize.pkl', 'wall_time']